{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matiaszabal/argand/blob/master/Copy_of_Ollama_ColabV4_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore the Advanced Ollama-Companion:\n",
        "# Streamlit Enhanced\n",
        "---  \n",
        "\n",
        "\n",
        "**Use this command to install local**:  \n",
        "\n",
        "\n",
        "```\n",
        "curl https://raw.githubusercontent.com/Luxadevi/Ollama-Companion/main/install.sh | sh  \n",
        "```\n",
        "\n",
        "  \n",
        "Welcome to the latest version of Ollama-Companion, now enhanced with Streamlit for a more interactive and intuitive user experience. As the developer behind this innovative project, I'm excited to introduce a suite of new features that redefine how you interact with and manage language models.\n",
        "\n",
        "Key Innovations in Ollama-Companion:\n",
        "1. Quantization of Huggingface Models via UI\n",
        "The foremost feature of Ollama-Companion is the ability to quantize Huggingface models through a user-friendly interface. This functionality allows you to efficiently convert models into different formats, catering to a variety of computational needs.\n",
        "\n",
        "2. Dynamic Module Integration\n",
        "Seamlessly integrate a variety of modules as defined in shared.py, ensuring a modular and scalable approach to application development.\n",
        "\n",
        "3. Streamlit-Powered User Interface\n",
        "The UI has been revamped using Streamlit to enhance intuitiveness and responsiveness, making it easier to navigate and interact with various features.\n",
        "\n",
        "4. Enhanced Model Interaction Features\n",
        "Modelfile Manager: Beyond model selection, this feature lets you delve into the details of each model, providing options to view, manage, and even delete model files as required.\n",
        "Interactive Modelfile Creator: Customize your model files in real-time, offering enhanced control and flexibility.\n",
        "5. Chat Interface with LLAVA Image Analysis\n",
        "The chat interface is equipped with LLAVA for image recognition and analysis, adding a dynamic and interactive dimension to your conversations with language models.\n",
        "\n",
        "6. Advanced Configuration Tools\n",
        "Ollama API Configurator: Manage Ollama API endpoints directly from the UI.\n",
        "LiteLLM Proxy and Public Endpoint: Easily set up proxies and public endpoints, ensuring secure and efficient model sharing.\n",
        "7. Efficient Model Management Systems\n",
        "Fast Model Downloading: Download models from Huggingface with improved speed.\n",
        "Quantization Options: Choose between high or medium precision GGUF formats for model transformation.\n",
        "Secure Model Uploads to Huggingface: Upload models to Huggingface confidently with enhanced security measures.\n",
        "8. Security Enhancements\n",
        "Token Encryption: Protect your Huggingface token with an additional layer of encryption for increased data security.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SIujJTVa7hAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to run\n",
        "---\n",
        "* Run the cells below by clicking the play button to download  and start ollama with a public url .  \n",
        "\n"
      ],
      "metadata": {
        "id": "3gQuQak3Ckh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/Luxadevi/Ollama-Companion/main/install.sh  && sudo chmod +x install.sh 2>&1 /dev/null\n",
        "#Sets up Latest version of Llama.cpp for quanting.\n",
        "#If You need to build a new one use the argument `-colab_compile`\n",
        "#Also interactive installer available use -interactive or -i\n",
        "#Using a virtual enviroment is not recommended within Google_Colab\n",
        "#Docs are embedded within Ollama-Companion\n",
        "!/content/install.sh -colab"
      ],
      "metadata": {
        "id": "gQSkK1w82y52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d0aba93-4e3e-4217-e1f5-ea09e0275ef7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceeding with colab installation.\n",
            "Starting Colab installation...\n",
            "This uses pre-compiled llama.cpp binaries.\n",
            "To freshly compile a new version, use -colab_compile.\n",
            "Refer to the llama.cpp GitHub repository for more info.\n",
            "Installing required packages...\n",
            "Cloning the Ollama Companion repository...\n",
            "Installing Python dependencies...\n",
            "Installing the HTTPX Python package...\n",
            "Downloading pre-compiled llama.cpp binaries...\n",
            "Extracting the downloaded binaries...\n",
            "Installing Ollama in headless mode...\n",
            "Logging installation type...\n",
            "Writing to log file...\n",
            "Companion successfully installed, you can launch next time with the start.sh script. Ollama-companion will autolaunch on port 8051 and defaults to making a public-facing URL for your companion. If you only want to run Ollama-companion locally: run the start.sh script with '-local' or '-lan' arguments.\n",
            "Terminated existing Ollama processes\n",
            "Starting Ollama-Companion with a public URL\n",
            "Starting Cloudflare Tunnel...\n",
            "Tunnel URL: https://gc-australian-lazy-patents.trycloudflare.com\n",
            "Starting Ollama\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.184.43.61:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2024-10-14 15:28:47.390 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 88, in exec_func_with_error_handling\n",
            "    result = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 579, in code_to_exec\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/Ollama-Companion/Homepage.py\", line 3, in <module>\n",
            "    from st_pages import Page, Section, show_pages, add_page_title, add_indentation\n",
            "ImportError: cannot import name 'show_pages' from 'st_pages' (/usr/local/lib/python3.10/dist-packages/st_pages/__init__.py)\n",
            "2024-10-14 15:30:35.751 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 88, in exec_func_with_error_handling\n",
            "    result = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 579, in code_to_exec\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/Ollama-Companion/pages/Upload_Converted_To_HF.py\", line 113, in <module>\n",
            "    high_precision_files = list_model_files(models_dir, \"High-Precision-Quantization\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/caching/cache_utils.py\", line 210, in __call__\n",
            "    return self._get_or_create_cached_value(args, kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/caching/cache_utils.py\", line 235, in _get_or_create_cached_value\n",
            "    return self._handle_cache_miss(cache, value_key, func_args, func_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/caching/cache_utils.py\", line 292, in _handle_cache_miss\n",
            "    computed_value = self._info.func(*func_args, **func_kwargs)\n",
            "  File \"/content/Ollama-Companion/pages/Upload_Converted_To_HF.py\", line 104, in list_model_files\n",
            "    models_dir_path = Path(models_dir)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 960, in __new__\n",
            "    self = cls._from_parts(args)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 594, in _from_parts\n",
            "    drv, root, parts = self._parse_args(args)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 578, in _parse_args\n",
            "    a = os.fspath(a)\n",
            "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
            "2024-10-14 15:30:44.488 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 88, in exec_func_with_error_handling\n",
            "    result = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 579, in code_to_exec\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/Ollama-Companion/Homepage.py\", line 3, in <module>\n",
            "    from st_pages import Page, Section, show_pages, add_page_title, add_indentation\n",
            "ImportError: cannot import name 'show_pages' from 'st_pages' (/usr/local/lib/python3.10/dist-packages/st_pages/__init__.py)\n",
            "2024-10-14 15:33:08.202 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 88, in exec_func_with_error_handling\n",
            "    result = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 579, in code_to_exec\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/Ollama-Companion/Homepage.py\", line 3, in <module>\n",
            "    from st_pages import Page, Section, show_pages, add_page_title, add_indentation\n",
            "ImportError: cannot import name 'show_pages' from 'st_pages' (/usr/local/lib/python3.10/dist-packages/st_pages/__init__.py)\n",
            "Requesting URL: http://35.184.43.61:8501/api/generate\n",
            "Headers: {'Content-Type': 'application/json'}\n",
            "Payload: {\n",
            "    \"model\":null,\n",
            "    \"prompt\":\"hola\"\n",
            "}\n",
            "2024-10-14 15:33:27.822 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 88, in exec_func_with_error_handling\n",
            "    result = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 579, in code_to_exec\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/Ollama-Companion/pages/Docs.py\", line 3, in <module>\n",
            "    from st_pages import Page,  add_indentation\n",
            "ImportError: cannot import name 'add_indentation' from 'st_pages' (/usr/local/lib/python3.10/dist-packages/st_pages/__init__.py)\n",
            "2024-10-14 15:35:30.935 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 199, in _new_conn\n",
            "    sock = connection.create_connection(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
            "    sock.connect(sa)\n",
            "TimeoutError: [Errno 110] Connection timed out\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
            "    response = self._make_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 495, in _make_request\n",
            "    conn.request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 441, in request\n",
            "    self.endheaders()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1278, in endheaders\n",
            "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1038, in _send_output\n",
            "    self.send(msg)\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 976, in send\n",
            "    self.connect()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 279, in connect\n",
            "    self.sock = self._new_conn()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 208, in _new_conn\n",
            "    raise ConnectTimeoutError(\n",
            "urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x79a3f6efd1e0>, 'Connection to 35.184.43.61 timed out. (connect timeout=None)')\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 667, in send\n",
            "    resp = conn.urlopen(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
            "    retries = retries.increment(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\", line 519, in increment\n",
            "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
            "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='35.184.43.61', port=8501): Max retries exceeded with url: /api/generate (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x79a3f6efd1e0>, 'Connection to 35.184.43.61 timed out. (connect timeout=None)'))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 88, in exec_func_with_error_handling\n",
            "    result = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 579, in code_to_exec\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/Ollama-Companion/pages/Chat_Interface.py\", line 144, in <module>\n",
            "    for response_chunk in stream_response(prompt, base_url, selected_model, encoded_images):\n",
            "  File \"/content/Ollama-Companion/pages/Chat_Interface.py\", line 46, in stream_response\n",
            "    with requests.post(url, json=payload, headers=headers, stream=True) as response:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/api.py\", line 115, in post\n",
            "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/api.py\", line 59, in request\n",
            "    return session.request(method=method, url=url, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 688, in send\n",
            "    raise ConnectTimeout(e, request=request)\n",
            "requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='35.184.43.61', port=8501): Max retries exceeded with url: /api/generate (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x79a3f6efd1e0>, 'Connection to 35.184.43.61 timed out. (connect timeout=None)'))\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If stopped or wanting to relaunch.**   \n",
        "Run this cell to restart your LLM stack"
      ],
      "metadata": {
        "id": "9QADK7qltjVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/Ollama-Companion/start.sh"
      ],
      "metadata": {
        "id": "aCwfO2OT6W_8",
        "outputId": "357ebf70-17b8-425c-f615-e7c009006b6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Terminated existing Ollama processes\n",
            "Starting Ollama-Companion with a public URL\n",
            "Starting Cloudflare Tunnel...\n",
            "Tunnel URL: https://baseball-area-ship-expanded.trycloudflare.com\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}